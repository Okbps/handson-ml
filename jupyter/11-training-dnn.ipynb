{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients\n",
    "Xavier and He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, (None, 3), name=\"X\")\n",
    "n_hidden1 = 300\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"Hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 1\n",
    "lx = np.linspace(-4, 4, 100)\n",
    "ly = list(map(lambda x: alpha*np.exp(x)-alpha if x<0 else x, lx))\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 4)\n",
    "# ax.xaxis.set_ticks_position('bottom')\n",
    "# ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "# set the x-spine (see below for more info on `set_position`)\n",
    "ax.spines['left'].set_position('zero')\n",
    "\n",
    "# turn off the right spine/ticks\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.yaxis.tick_left()\n",
    "\n",
    "# set the y-spine\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "\n",
    "# turn off the top spine/ticks\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.tick_bottom()\n",
    "\n",
    "\n",
    "ax.plot(lx, ly)\n",
    "ax.set_title(\"ELU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid = X_test[:5000]\n",
    "y_valid = y_test[:5000]\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "threshold = 1.0\n",
    "n_batches = X_train.shape[0]//batch_size\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield (X_batch, y_batch)\n",
    "        \n",
    "def get_logdir():\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    return \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.905\n",
      "1 Test accuracy: 0.9287\n",
      "2 Test accuracy: 0.937\n",
      "3 Test accuracy: 0.9434\n",
      "4 Test accuracy: 0.952\n",
      "5 Test accuracy: 0.954\n",
      "6 Test accuracy: 0.956\n",
      "7 Test accuracy: 0.959\n",
      "8 Test accuracy: 0.9585\n",
      "9 Test accuracy: 0.9614\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(input=False, shape=(), name=\"training\")\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)\n",
    "#     training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float16))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    \n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        \n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "        save_path = saver.save(sess, \"tmp/bn_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"tmp/mnist_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"train/GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"important_ops\", op)\n",
    "    \n",
    "X, y, accuracy, training_op = tf.get_collection(\"important_ops\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/mnist_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"tmp/mnist_model_final.ckpt\")\n",
    "    save_path = saver.save(sess, \"tmp/hidden123_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing the Lower Layers: fetch selected vars to optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss/loss:0\")\n",
    "\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing the Lower Layers: add \"stop gradient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "#     hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2, 50, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, 25, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "    \n",
    "saver = tf.train.Saver()    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3198201\n",
      "Test loss: 0.24075203\n",
      "Test loss: 0.19596633\n",
      "Test loss: 0.16589253\n",
      "Test loss: 0.1499387\n",
      "Test loss: 0.13555607\n",
      "Test loss: 0.12521482\n",
      "Test loss: 0.121826746\n",
      "Test loss: 0.12452241\n",
      "Test loss: 0.10719646\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        \n",
    "        loss_test = sess.run(loss, feed_dict={X:X_test, y:y_test})\n",
    "        print(\"Epoch:\", epoch, \"Test loss:\", loss_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"tmp/layers_x4_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# exporting saved graph to tensorboard\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf.train.import_meta_graph(\"tmp/mnist_model_final.ckpt.meta\")\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    tf.train.write_graph(tf.get_default_graph(), \"tf_logs\", \"restored-mnist\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf.train.import_meta_graph(\"tmp/layers_x4_mnist_model_final.ckpt.meta\")\n",
    "tf.get_default_graph().get_operations()\n",
    "# tf.get_default_graph().get_tensor_by_name(\"train/GradientDescent/update_outputs/bias/ApplyGradientDescent:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/layers_x4_mnist_model_final.ckpt\n",
      "Epoch: 0 Test loss: 0.10686793\n",
      "Epoch: 1 Test loss: 0.10657914\n",
      "Epoch: 2 Test loss: 0.10632399\n",
      "Epoch: 3 Test loss: 0.10609763\n",
      "Epoch: 4 Test loss: 0.10589534\n",
      "Epoch: 5 Test loss: 0.105713986\n",
      "Epoch: 6 Test loss: 0.10555039\n",
      "Epoch: 7 Test loss: 0.105402514\n",
      "Epoch: 8 Test loss: 0.10526825\n",
      "Epoch: 9 Test loss: 0.10514573\n",
      "Epoch: 10 Test loss: 0.10503374\n",
      "Epoch: 11 Test loss: 0.10493043\n",
      "Epoch: 12 Test loss: 0.104835495\n",
      "Epoch: 13 Test loss: 0.104747646\n",
      "Epoch: 14 Test loss: 0.10466616\n",
      "Epoch: 15 Test loss: 0.1045906\n",
      "Epoch: 16 Test loss: 0.1045198\n",
      "Epoch: 17 Test loss: 0.10445415\n",
      "Epoch: 18 Test loss: 0.10439314\n",
      "Epoch: 19 Test loss: 0.10433577\n",
      "Epoch: 20 Test loss: 0.10428163\n",
      "Epoch: 21 Test loss: 0.1042312\n",
      "Epoch: 22 Test loss: 0.10418347\n",
      "Epoch: 23 Test loss: 0.10413814\n",
      "Epoch: 24 Test loss: 0.10409582\n",
      "Epoch: 25 Test loss: 0.10405599\n",
      "Epoch: 26 Test loss: 0.104018286\n",
      "Epoch: 27 Test loss: 0.10398276\n",
      "Epoch: 28 Test loss: 0.103948794\n",
      "Epoch: 29 Test loss: 0.10391723\n",
      "Epoch: 30 Test loss: 0.103887\n",
      "Epoch: 31 Test loss: 0.10385847\n",
      "Epoch: 32 Test loss: 0.10383115\n",
      "Epoch: 33 Test loss: 0.10380525\n",
      "Epoch: 34 Test loss: 0.10378086\n",
      "Epoch: 35 Test loss: 0.103757724\n",
      "Epoch: 36 Test loss: 0.10373596\n",
      "Epoch: 37 Test loss: 0.10371508\n",
      "Epoch: 38 Test loss: 0.10369573\n",
      "Epoch: 39 Test loss: 0.10367657\n",
      "Epoch: 40 Test loss: 0.10365829\n",
      "Epoch: 41 Test loss: 0.10364136\n",
      "Epoch: 42 Test loss: 0.103625305\n",
      "Epoch: 43 Test loss: 0.103609726\n",
      "Epoch: 44 Test loss: 0.10359525\n",
      "Epoch: 45 Test loss: 0.103580676\n",
      "Epoch: 46 Test loss: 0.10356766\n",
      "Epoch: 47 Test loss: 0.10355487\n",
      "Epoch: 48 Test loss: 0.10354289\n",
      "Epoch: 49 Test loss: 0.10353102\n",
      "Epoch: 50 Test loss: 0.103520095\n",
      "Epoch: 51 Test loss: 0.10350979\n",
      "Epoch: 52 Test loss: 0.103499316\n",
      "Epoch: 53 Test loss: 0.10348955\n",
      "Epoch: 54 Test loss: 0.103480615\n",
      "Epoch: 55 Test loss: 0.10347117\n",
      "Epoch: 56 Test loss: 0.10346323\n",
      "Epoch: 57 Test loss: 0.103455484\n",
      "Epoch: 58 Test loss: 0.10344756\n",
      "Epoch: 59 Test loss: 0.10344043\n",
      "Epoch: 60 Test loss: 0.10343325\n",
      "Epoch: 61 Test loss: 0.10342659\n",
      "Epoch: 62 Test loss: 0.103419535\n",
      "Epoch: 63 Test loss: 0.10341292\n",
      "Epoch: 64 Test loss: 0.1034066\n",
      "Epoch: 65 Test loss: 0.103400685\n",
      "Epoch: 66 Test loss: 0.10339502\n",
      "Epoch: 67 Test loss: 0.10338962\n",
      "Epoch: 68 Test loss: 0.103384435\n",
      "Epoch: 69 Test loss: 0.103379495\n",
      "Epoch: 70 Test loss: 0.10337451\n",
      "Epoch: 71 Test loss: 0.10337031\n",
      "Epoch: 72 Test loss: 0.10336675\n",
      "Epoch: 73 Test loss: 0.10336231\n",
      "Epoch: 74 Test loss: 0.103358254\n",
      "Epoch: 75 Test loss: 0.10335462\n",
      "Epoch: 76 Test loss: 0.10335051\n",
      "Epoch: 77 Test loss: 0.10334703\n",
      "Epoch: 78 Test loss: 0.103343725\n",
      "Epoch: 79 Test loss: 0.10334026\n",
      "Epoch: 80 Test loss: 0.103336826\n",
      "Epoch: 81 Test loss: 0.10333391\n",
      "Epoch: 82 Test loss: 0.103330776\n",
      "Epoch: 83 Test loss: 0.10332764\n",
      "Epoch: 84 Test loss: 0.10332522\n",
      "Epoch: 85 Test loss: 0.103322715\n",
      "Epoch: 86 Test loss: 0.103320114\n",
      "Epoch: 87 Test loss: 0.10331766\n",
      "Epoch: 88 Test loss: 0.103315525\n",
      "Epoch: 89 Test loss: 0.103312954\n",
      "Epoch: 90 Test loss: 0.103310764\n",
      "Epoch: 91 Test loss: 0.10330869\n",
      "Epoch: 92 Test loss: 0.103306435\n",
      "Epoch: 93 Test loss: 0.10330435\n",
      "Epoch: 94 Test loss: 0.10330233\n",
      "Epoch: 95 Test loss: 0.10330077\n",
      "Epoch: 96 Test loss: 0.103298984\n",
      "Epoch: 97 Test loss: 0.10329703\n",
      "Epoch: 98 Test loss: 0.10329512\n",
      "Epoch: 99 Test loss: 0.10329369\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.train.import_meta_graph(\"tmp/layers_x4_mnist_model_final.ckpt.meta\")\n",
    "\n",
    "restore_saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"tmp/layers_x4_mnist_model_final.ckpt\")\n",
    "    \n",
    "    hidden2 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden2/Relu:0\")\n",
    "    X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "    y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "    training_op = tf.get_default_graph().get_tensor_by_name(\"train/GradientDescent/update_outputs/bias/ApplyGradientDescent:0\")\n",
    "    loss = tf.get_default_graph().get_tensor_by_name(\"loss/Mean:0\")\n",
    "    \n",
    "    h2_cache = sess.run([hidden2], feed_dict={X: X_train})\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(X_train.shape[0])\n",
    "        hidden2_batches = np.array_split(h2_cache[0][shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "            \n",
    "        loss_test = sess.run(loss, feed_dict={X:X_test, y:y_test})\n",
    "        print(\"Epoch:\", epoch, \"Test loss:\", loss_test)            \n",
    "            \n",
    "#     save_path = saver.save(sess, \"tmp/cached_mnist_model_final.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
